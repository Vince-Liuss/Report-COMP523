\documentclass[onecolumn]{report}
\usepackage{sharina}

\begin{document}
\title{Summary of COMP523 Advanced Algorithm}
\maketitle
\chapter{Symmetry Notation}
\section{Asymptotic Notation}
Asymptotic notation is a way of describing the limiting behavior of a function when the argument tends towards a particular value or infinity. In computer science, asymptotic notation is frequently used to describe the running time or space usage of an algorithm.\\
\begin{itemize}
\item $O$-notation: $f(n) = O(g(n))$ if there exist constants $c$ and $n_0$ such that $0 \leq f(n) \leq cg(n)$ for all $n \geq n_0$.
\item $\Omega$-notation: $f(n) = \Omega(g(n))$ if there exist constants $c$ and $n_0$ such that $0 \leq cg(n) \leq f(n)$ for all $n \geq n_0$.
\item $\Theta$-notation: $f(n) = \Theta(g(n))$ if there exist constants $c_1$, $c_2$ and $n_0$ such that $0 \leq c_1g(n) \leq f(n) \leq c_2g(n)$ for all $n \geq n_0$.
\item $o$-notation: $f(n) = o(g(n))$ if for any constant $c > 0$, there exists a constant $n_0$ such that $0 \leq f(n) < cg(n)$ for all $n \geq n_0$.
\item $\omega$-notation: $f(n) = \omega(g(n))$ if for any constant $c > 0$, there exists a constant $n_0$ such that $0 \leq cg(n) < f(n)$ for all $n \geq n_0$.
\end{itemize}

\section{Comparing Functions}
\subsection{Transitivity}
\begin{itemize}
\item $f(n) = O(g(n))$ and $g(n) = O(h(n))$ implies $f(n) = O(h(n))$.
\item $f(n) = \Omega(g(n))$ and $g(n) = \Omega(h(n))$ implies $f(n) = \Omega(h(n))$.
\item $f(n) = \Theta(g(n))$ and $g(n) = \Theta(h(n))$ implies $f(n) = \Theta(h(n))$.
\end{itemize}
For example, $n^2 = O(n^3)$ and $n^3 = O(n^4)$ implies $n^2 = O(n^4)$.

\subsection{Reflexivity}
\begin{itemize}
\item $f(n) = O(f(n))$.
\item $f(n) = \Omega(f(n))$.
\item $f(n) = \Theta(f(n))$.
\end{itemize}
For example, $n^2 = O(n^2)$.

\subsection{Symmetry}
\begin{itemize}
\item $f(n) = O(g(n))$ implies $g(n) = O(f(n))$.
\item $f(n) = \Omega(g(n))$ implies $g(n) = \Omega(f(n))$.
\item $f(n) = \Theta(g(n))$ implies $g(n) = \Theta(f(n))$.
\item $f(n) = o(g(n))$ implies $g(n) = \omega(f(n))$.
\item $f(n) = \omega(g(n))$ implies $g(n) = o(f(n))$.
\end{itemize}
For example, $n^2 = O(n^3)$ implies $n^3 = \Omega(n^2)$.

\subsection{Transpose Symmetry}
\begin{itemize}
\item $f(n) = O(g(n))$ if and only if $g(n) = \Omega(f(n))$.
\item $f(n) = \Theta(g(n))$ if and only if $g(n) = \Theta(f(n))$.
\item $f(n) = o(g(n))$ if and only if $g(n) = \omega(f(n))$.
\item $f(n) = \omega(g(n))$ if and only if $g(n) = o(f(n))$.
\end{itemize}
For example, $n^2 = O(n^3)$ if and only if $n^3 = \Omega(n^2)$.\\

\subsection{sum and maximum}
\begin{equation*}
    f_1(n) + f_2(n)+\dots +f_k(n) = \Theta(\max(f_1(n), f_2(n), \dots, f_k(n)))
\end{equation*}
where $k$ is a constant positive integer.\\
Let $f_j(n) = j$, $k=n$, then
\begin{equation*}
    f_1(n) + f_2(n)+\dots +f_k(n) = n(n+1)/2 = \Theta(n^2)
\end{equation*}

\subsection{Running time hierarchy}
\begin{itemize}
    \item logarithmic: $O(\log n)$
    \item linear: $O(n)$
    \item $n\log n$: $O(n\log n)$
    \item quadratic: $O(n^2)$
    \item polynomial: $O(n^k)$
    \item exponential: $O(c^n)$
    \item constant: $O(1)$
    \item superconstant: $\omega (1)$
    \item sublinear: $o(n)$
    \item superlinear: $\omega (n)$
    \item superpolynomial: $\omega (n^k)$
    \item subexponential: $o(c^n)$
\end{itemize}

\section{Expect of algorithms}
\textbf{Correctness}: An algorithm is correct if it halts with the correct output for every input instance.\\
\textbf{Termination}: An algorithm is terminating if it halts for every input instance.\\
\textbf{Efficiency}: An algorithm is efficient if it halts with the correct output for every input instance and runs in polynomial time.\\

\chapter{Recursion and Divide and Conquer techniques}
\section{Finding Majority in array}
The pesudocode of the algorithm is shown in Algorithm \ref{alg:majority}.
\begin{algorithm}
\label{alg:majority}
\caption{Finding Majority in array}
\begin{algorithmic}[1]
\Procedure{Majority}{$A$}
\State $n \gets \text{length of } A$
\If {$n = 0$}
\State \textbf{return} $-1$
\EndIf
\If {$n = 1$}
\State \textbf{return} $A[1]$
\EndIf
\If {$n \large 1$ and $n$ is odd}
\State 
\EndIf
\State Array $B$ of size $n/2$
\State set $j$=0
\For {$i=1$ to $n/2$}
\If {$A[2i-1] = A[2i]$}
\State $B[j] \gets A[2i-1]$
\State $j \gets j+1$
\EndIf
\EndFor
\State $m \gets \textsc{Majority}(B)$
\State $count \gets 0$
\For {$i=1$ to $n$}
\If {$A[i] = m$}
\State $count \gets count+1$
\EndIf
\EndFor
\If {$count > n/2$}
\State \textbf{return} $m$
\Else
\State \textbf{return} $-1$
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}\\
\textbf{Correctness}:\\
Lemma: If $A$ has a majority element, then the majority element of $A$ is also the majority element of $B$.\\
Base case: $n=1$, the majority element is $A[1]$.\\
Induction hypothesis: Assume that the lemma is true for $n=k$, we will prove that the lemma is true for $n=k+1$.\\
Induction step: If $A$ has a majority element, then the majority element of $A$ is also the majority element of $B$.\\
Case 1 ($A$ has a majority element $m$): Then by the lemma, it is also the majority element of $B$. Then $m$ appears more than $k/2$ times in $B$. Then $m$ appears more than $(k+1)/2$ times in $A$.\\
Case 2 ($A$ has no majority element): Then $B$ has no majority element. Then $A$ has no majority element.\\
\textbf{Proof the lemma}:\\
proof by contradiction. Assume that $A$ has a majority element $m$ and $B$ has a majority element $m'$, but $m \neq m'$. \\
Let $x$ be the numbers of occurrence of $m$ in $A$.\\
Let $y$ be the numbers of occurrence of $m'$ in $B$.\\
Then $2y$ times from pairs that are represented in $B$ by a value different from $m'$, and $x-2y$ times, since each occurrence of $m$ in $A$ that is not paired with another occurrence of $m$ in $A$ is paired with an occurrence of $m'$ in $B$.\\
In total, this gives $2y+x-2y=x$ occurrences of $m$ in $A$, which is a contradiction.\\
\textbf{Running time}:\\
Recursive formula for the running time:
\begin{equation*}
    T(n) \leq T(n/2) + cn
\end{equation*}
where $c$ is a constant.\\
The solution to the recurrence is $T(n) = O(n)$.\\

\section{Searching in logarithmic time}
Searching faster with BinarySearch.\\
It is a particular case of the divide-and-conquer paradigm.\\
\textbf{Input}: A sorted array $A$ of $n$ elements and a value $x$.\\
\textbf{Output}: An index $i$ such that $A[i] = x$ or the special value $-1$ if $x$ does not appear in $A$.\\
\textbf{Pseudocode} is shown in Algorithm \ref{alg:binarysearch}.
\begin{algorithm}
\label{alg:binarysearch}
\caption{BinarySearch}
\begin{algorithmic}[1]
\Procedure{BinarySearch}{$x,i,j$}
\If {$i=j$}
\If {$A[i] = x$}
\State \textbf{return} $i$
\Else
\State \textbf{return} $-1$
\EndIf
\Else
\If {$x=A[\lfloor (i+j)/2 \rfloor]$}
\State \textbf{return} $\lfloor (i+j)/2 \rfloor$
\ElsIf {$x<A[\lfloor (i+j)/2 \rfloor]$}
\State \textbf{return} $\textsc{BinarySearch}(x,i,\lfloor (i+j)/2 \rfloor)$
\Else
\State \textbf{return} $\textsc{BinarySearch}(x,\lfloor (i+j)/2 \rfloor+1,j)$
\EndIf
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}\\
\textbf{Running time}:\\
The number of comparisons performed by BinarySearch is:\\
\begin{equation*}
    T(n) \leq  T(n/2) + 4  
\end{equation*}
Keep calculate:
\begin{align*}
    T(n) &\leq T(n/2) + 4\\
    &\leq T(n/4) + 4 + 4\\
    &\leq T(n/8) + 4 + 4 + 4\\
    &\leq T(n/2^k) + 4k\\
    &\leq T(n/2^{\log (n-1)}) + 4\log (n-1)\\
    &= T(2) + 4(\log n-1)\\
    &\leq 4\log n - 4\\
    &= 4\log n\\
\end{align*}
proof $T(n) \leq 4\log n$:\\
Base case: $n=1$, $T(1) = 0 \leq 4\log 1 = 0$.\\
Induction hypothesis: Assume that the lemma is true for $n=k$, we will prove that the lemma is true for $n=k+1$.\\
Induction step: $T(k+1) \leq 4\log (k+1)$.\\
\begin{align*}
    T(k+1) &\leq T(k/2) + 4\\
    &\leq 4\log (k/2) + 4\\
    &= 4\log k - 4 + 4\\
    &= 4\log k\\
    &\leq 4\log (k+1)
\end{align*}
\textbf{Memory usage}:\\
The memory usage of BinarySearch is:\\
\begin{equation*}
    M(n) = O(\log n)
\end{equation*}
\textbf{Comparing BinarySearch and LinearSearch}:\\
\begin{equation*}
    T_{\text{BinarySearch}}(n) = O(\log n)
\end{equation*}
\begin{equation*}
    T_{\text{LinearSearch}}(n) = O(n)
\end{equation*}
\begin{equation*}
    T_{\text{BinarySearch}}(n) = O(\log n) < O(n) = T_{\text{LinearSearch}}(n)  
\end{equation*}
\begin{equation*}
    M_{\text{BinarySearch}}(n) = O(\log n) < O(1) = M_{\text{LinearSearch}}(n)
\end{equation*}

\section{Running time of Divide and Conquer algorithms}
The Master Theorem:\\
Suppose that $T(n)$ satisfies the recurrence:
\begin{equation*}
    T(n) \leq aT(n/b) + cn^d
\end{equation*}
where $a \geq 1$, $b > 1$, $c > 0$ and $d \geq 0$ are constants.\\
Then $T(n)$ has the following asymptotic bounds:\\
\begin{equation*}
    T(n) = \begin{cases}
    O(n^d) & \text{if } d > \log_b a\\
    O(n^d\log n) & \text{if } d = \log_b a\\
    O(n^{\log_b a}) & \text{if } d < \log_b a
    \end{cases}
\end{equation*}
This theorem is useful for solving recurrences of the form:
\begin{equation*}
    T(n) = aT(n/b) + f(n)
\end{equation*}
where $a \geq 1$, $b > 1$ and $f(n)$ is an asymptotically positive function.\\
\textbf{Example}:\\
\begin{equation*}
    T(n) = 8T(n/2) + 100n^2
\end{equation*}
$a = 8$, $b = 2$, $f(n) = 100n^2$, $d = 2$, $\log_b a = \log_2 8 = 3$.\\
$d = 2 < \log_b a = 3$, so $T(n) = O(n^{\log_b a}) = O(n^3)$.\\

\section{Finding piar of points closest to each other}
\textbf{Input}: A set $P$ of $n$ points in the plane.\\
\textbf{Output}: The pair of points in $P$ that are closest to each other.\\
\textbf{Pseudocode} is shown in Algorithm \ref{alg:closestpair}.
\begin{algorithm}
\label{alg:closestpair}
\caption{ClosestPair}
\begin{algorithmic}[1]
\Procedure{ClosestPair}{$P_1,\dots,P_n$}
\State Construct $P_x$ and $P_y$. $P_x$ is sorted by $x$-coordinate, $P_y$ is sorted by $y$-coordinate.
\State \textbf{return} $\textsc{ClosestPairRec}(P_x,P_y)$
\EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{ClosestPairRec}
\begin{algorithmic}[1]
\Procedure{ClosestPairRec}{$P_x,P_y$}
\If {$|P_x|=|P_y| \leq 3$}
\State For each pair of points $(P_i,P_j)$, compute $d(P_i,P_j)$
\State \textbf{return} the pair of points with the smallest distance
\EndIf
\State Construct $Q_x$, $Q_y$, $R_x$ and $R_y$.
\State ($l_1,l_2$) = \textsc{ClosestPairRec}($Q_x,Q_y$)
\State ($r_1,r_2$) = \textsc{ClosestPairRec}($R_x,R_y$)
\State $\delta = \min\{d(l_1,l_2),d(r_1,r_2)\}$
\State $x^*$ = the largest $x$-coordinate in $Q_x$
\State $L = {(x,y):x=x^*}$
\State $S = \{p \in P: p \in L \text{ and } p \text{ is within } \delta \text{ of } L\}$
\State Construct $S_v$ 
\For {$p \in S$}
\State Let $q$ be the point in $S_v$ closest to $p$
\If {$d(p,q) < \delta$}
\State $\delta = d(p,q)$
\State ($s_1,s_2$) = ($p,q$)
\EndIf
\EndFor
\If {$d(s_1,s_2) < \min\{d(l_1,l_2),d(r_1,r_2)\}$}
\State \textbf{return} ($s_1,s_2$)
\EndIf
\If {$d(l_1,l_2) < d(r_1,r_2)$}
\State \textbf{return} ($l_1,l_2$)
\Else
\State \textbf{return} ($r_1,r_2$)
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}
\textbf{Running time}:\\
$T(n) \leq 2T(n/2) + O(n\log n) = O(n\log n)$\\
\textbf{Example}:\\

\chapter{Graph Algorithms}
\section{Graph Definitions}
\textbf{Graph}: A graph $G$ consists of a set $V$ of vertices and a set $E$ of edges, where each edge is associated with a pair of vertices.\\
\textbf{Directed Graph}: A directed graph $G$ consists of a set $V$ of vertices and a set $E$ of directed edges, where each directed edge is associated with an ordered pair of vertices.\\
\textbf{Undirected Graph}: An undirected graph $G$ consists of a set $V$ of vertices and a set $E$ of undirected edges, where each undirected edge is associated with an unordered pair of vertices.\\
\textbf{Neighbours of a vertex $v$}: Set of vertices that are connected to $v$ by an edge.\\
\textbf{Degree of a vertex $v$}: number of neighbours of $v$, denoted by $deg(v)$.\\
\textbf{Path}: A sequence of (non-repeating) nodes with consecutive nodes being connected by an edge.\\
length = node count - 1 = edge count.\\
\textbf{Distance between two nodes}: The number of edges in the shortest path between the two nodes.\\
\textbf{Graph diameter}: The maximum distance between any two nodes in the graph.\\

\textbf{Lines,cycles,trees and cliques}:\\
\textbf{Line}: A graph with $n$ vertices and $n-1$ edges.\\
\textbf{Cycle}: A graph with $n$ vertices and $n$ edges.\\
\textbf{cliques}: A graph with $n$ vertices and $n(n-1)/2$ edges.\\
\textbf{Tree}: A graph with $n$ vertices and $n-1$ edges.\\

\textbf{Graph representations}:\\
\textbf{Adjacency matrix}: A $n \times n$ matrix $A$ where $A_{ij} = 1$ if there is an edge between $i$ and $j$, and $A_{ij} = 0$ otherwise.\\
examples of adjacency matrices:\\
Given the following graph:\\
\begin{center}
    \begin{tikzpicture}
\node[shape=circle,draw=black] (A) at (0,0) {0};
\node[shape=circle,draw=black] (B) at (2,0) {1};
\node[shape=circle,draw=black] (C) at (2,2) {2};
\node[shape=circle,draw=black] (D) at (0,2) {3};
\path [-] (A) edge node[left] {} (B);
\path [-] (B) edge node[left] {} (C);
\path [-] (C) edge node[left] {} (D);
\path [-] (D) edge node[left] {} (A);
\path [-] (A) edge node[left] {} (C);
\end{tikzpicture}\\
\end{center}
The adjacency matrix is:\\
\begin{equation*}
    \begin{bmatrix}
    0 & 1 & 1 & 1\\
    1 & 0 & 1 & 0\\
    1 & 1 & 0 & 1\\
    1 & 0 & 1 & 0
    \end{bmatrix}
\end{equation*}
\textbf{Adjacency matrix for directed graphs}: A $n \times n$ matrix $A$ where $A_{ij} = 1$ if there is an edge from $i$ to $j$, and $A_{ij} = 0$ otherwise.\\
examples of adjacency matrices for directed graphs:\\
Given the following graph:\\
\begin{center}
\begin{tikzpicture}
\node[shape=circle,draw=black] (A) at (0,0) {0};
\node[shape=circle,draw=black] (B) at (2,0) {1};
\node[shape=circle,draw=black] (C) at (2,2) {2};
\node[shape=circle,draw=black] (D) at (0,2) {3};
\path [->] (A) edge node[left] {} (B);
\path [->] (B) edge node[left] {} (C);
\path [->] (C) edge node[left] {} (D);
\path [->] (D) edge node[left] {} (A);
\path [->] (A) edge node[left] {} (C);
\end{tikzpicture}\\
\end{center}
The adjacency matrix is:\\
\begin{equation*}
    \begin{bmatrix}
    0 & 1 & 0 & 0\\
    0 & 0 & 1 & 0\\
    0 & 0 & 0 & 1\\
    1 & 0 & 0 & 0
    \end{bmatrix}
\end{equation*}
\textbf{Adjacency list}: A list of lists, where the $i$th list contains the neighbours of vertex $i$.\\
Given the following graph:\\
\begin{center}
\begin{tikzpicture}
\centering
\node[shape=circle,draw=black] (A) at (0,0) {0};
\node[shape=circle,draw=black] (B) at (2,0) {1};
\node[shape=circle,draw=black] (C) at (2,2) {2};
\node[shape=circle,draw=black] (D) at (0,2) {3};
\path [-] (A) edge node[left] {} (B);
\path [-] (B) edge node[left] {} (C);
\path [-] (C) edge node[left] {} (D);
\path [-] (D) edge node[left] {} (A);
\path [-] (A) edge node[left] {} (C);
\end{tikzpicture}\\
\end{center}
The adjacency list is:\\
\begin{equation*}
    \begin{bmatrix}
    1 & 2 & 3\\
    0 & 2\\
    0 & 1 & 3\\
    0 & 2
    \end{bmatrix}
\end{equation*}
\textbf{Adjacency list for directed graphs}: A list of lists, where the $i$th list contains the neighbours of vertex $i$.\\
Given the following graph:\\
\begin{center}
    \begin{tikzpicture}
\centering
\node[shape=circle,draw=black] (A) at (0,0) {0};
\node[shape=circle,draw=black] (B) at (2,0) {1};
\node[shape=circle,draw=black] (C) at (2,2) {2};
\node[shape=circle,draw=black] (D) at (0,2) {3};
\path [->] (A) edge node[left] {} (B);
\path [->] (B) edge node[left] {} (C);
\path [->] (C) edge node[left] {} (D);
\path [->] (D) edge node[left] {} (A);
\path [->] (A) edge node[left] {} (C);
\end{tikzpicture}\\
\end{center}

The adjacency list is:\\
\begin{equation*}
    \begin{bmatrix}
    1 & 2\\
    2\\
    3\\
    0
    \end{bmatrix}
\end{equation*}
\textbf{Adjacency matrix vs adjacency list}:\\
\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
Adjacency matrix & Adjacency list\\
\hline
$O(1)$ to check if there is an edge between $i$ and $j$ & $O(min(deg(i),deg(j)))$ to check if there is an edge between $i$ and $j$\\
\hline
$O(n)$ to find the neighbours of $i$ & $O(deg(j))$ to find the neighbours of $i$\\
\hline
$O(n^2)$ space & $O(n+m)$ space\\
\hline
\end{tabular}
\end{table}

\section{Depth-first search}
\textbf{Depth-first search}: A graph search algorithm that explores the neighbours of a vertex before exploring the neighbours of its neighbours.\\
example of depth-first search:\\
\begin{center}
    \begin{tikzpicture}
\centering
\node[shape=circle,draw=black] (A) at (0,0) {0};
\node[shape=circle,draw=black] (B) at (2,0) {1};
\node[shape=circle,draw=black] (C) at (2,2) {2};
\node[shape=circle,draw=black] (D) at (0,2) {3};
\node[shape=circle,draw=black] (E) at (4,0) {4};
\node[shape=circle,draw=black] (F) at (4,2) {5};
\path [-] (A) edge node[left] {} (B);
\path [-] (B) edge node[left] {} (C);
\path [-] (C) edge node[left] {} (D);
\path [-] (D) edge node[left] {} (A);
\path [-] (A) edge node[left] {} (C);
\path [-] (E) edge node[left] {} (B);
\path [-] (F) edge node[left] {} (C);
\end{tikzpicture}\\
\end{center}
The depth-first search sequence is:\\
\begin{equation*}
    0,1,2,3,5,4
\end{equation*}
\textbf{Depth-first search algorithm}:\\
\begin{algorithm}[H]
\caption{Depth-first search algorithm}
\begin{algorithmic}[1]
\Procedure{DFS}{$G,v$}
\For {$e \in V$}
\If {$e$ is unexplored}
\State $u$ = head of $e$
\If {$u$ is unexplored}
\State $e$ is a tree edge
\State \Call{DFS}{$G,u$}
\Else
\State $e$ is a back edge
\EndIf
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
\textbf{Running time of depth-first search}: $O(n+m)$\\


\section{Breadth-first search}
\textbf{Breadth-first search}: A graph search algorithm that explores the neighbours of a vertex before exploring the neighbours of its neighbours.\\
exaqmple of breadth-first search:\\
\begin{center}
    \begin{tikzpicture}
\centering
\node[shape=circle,draw=black] (A) at (0,0) {0};
\node[shape=circle,draw=black] (B) at (2,0) {1};
\node[shape=circle,draw=black] (C) at (2,2) {2};
\node[shape=circle,draw=black] (D) at (0,2) {3};
\node[shape=circle,draw=black] (E) at (4,0) {4};
\node[shape=circle,draw=black] (F) at (4,2) {5};
\path [-] (A) edge node[left] {} (B);
\path [-] (B) edge node[left] {} (C);
\path [-] (C) edge node[left] {} (D);
\path [-] (D) edge node[left] {} (A);
\path [-] (A) edge node[left] {} (C);
\path [-] (E) edge node[left] {} (B);
\path [-] (F) edge node[left] {} (C);
\end{tikzpicture}\\
\end{center}
The breadth-first search sequence starting from vertex 0 is 0, 1, 2, 3, 4, 5.\\
\textbf{Breadth-first search algorithm}:\\
\begin{algorithm}[H]
\caption{Breadth-first search algorithm}
\begin{algorithmic}[1]
\Procedure{BFS}{$G,s$}
\State initial empty list $L$
\State $L \gets {s}$
\State $i \gets 0$
\While{$L[i] \neq \emptyset$}
\State $L_{i+1} \gets empty list$
\For{$v \in L[i]$}
\For{edges $(e)$ incident to $v$}
\If{$e$ is unexplored}
\State $w \gets$ the other end of $e$
\If {$w$ is unexplored}
\State label $e$ as a tree edge
\State add $w$ to $L_{i+1}$
\Else
\State label $e$ as a cross edge
\EndIf
\EndIf
\EndFor
\EndFor
\State $i \gets i+1$
\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}
\textbf{Running time of breadth-first search}: $O(n+m)$\\


\section{Strong Connectivity}
\textbf{Directed graph}: A graph where the edges have a direction.\\
Examples:\\
\begin{center}
    \begin{tikzpicture}
\centering
\node[shape=circle,draw=black] (A) at (0,0) {0};
\node[shape=circle,draw=black] (B) at (2,0) {1};
\node[shape=circle,draw=black] (C) at (2,2) {2};
\node[shape=circle,draw=black] (D) at (0,2) {3};
\node[shape=circle,draw=black] (E) at (4,0) {4};
\node[shape=circle,draw=black] (F) at (4,2) {5};
\path [->] (A) edge node[left] {} (B);
\path [->] (B) edge node[left] {} (C);
\path [->] (C) edge node[left] {} (D);
\path [->] (D) edge node[left] {} (A);
\path [->] (A) edge node[left] {} (C);
\path [->] (E) edge node[left] {} (B);
\path [->] (F) edge node[left] {} (C);
\end{tikzpicture}\\
\end{center}
\noindent
\textbf{DFS and BFS on directed graphs}:\\
Very similar to undirected graphs, except that we only consider edges that go out of a vertex.\\
Running time is $O(n+m)$\\
For example graph above the DFS sequence is 0, 1, 2, 3.\\
The BFS sequence is 0, 1, 2, 3.\\
\subsection{Connectivity}
\textbf{Weak connectivity}:If we ignore the direction for all edges, there would be a pah from any vertex to any other vertex.\\
\textbf{Strong Connectivity}:For every two nodes $u$ and $v$, there is a path from $u$ to $v$ and a path from $v$ to $u$.\\

\subsection{Mutual Reachability}
Two nodes $u$ and $v$ are mutually reachable if there is a path from $u$ to $v$ and a path from $v$ to $u$.\\
\textbf{Strong connectivity}:For every pair of nodes $u$ and $v$, these two nodes are mutually reachable.\\
\textbf{Transitivity}:If $u$ is mutually reachable with $v$ and $v$ is mutually reachable with $w$, then $u$ is mutually reachable with $w$.\\
\\


\subsection{Testing strong connectivity}
\begin{algorithm}[H]
\caption{Testing strong connectivity}
\begin{algorithmic}[1]
\Procedure{TestStrongConnectivity}{$G$}
\State define $G^R$ to be the graph with the same vertices as $G$ but with all edges reversed
\State Select a node $s$ in $G$
\State BFS($G,s$),BFS($G^R,s$)
\For{each node $v$}
\If{$v$ is unexplored in either BFS}
\State \Return False
\EndIf
\EndFor
\State \Return True
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Testing bipartiteness}
\textbf{Bipartite graph}: A graph $G=(V,E)$ is bipartite if any only if the vertices can be partitioned into two sets $V_1$ and $V_2$ such that every edge has one end in $V_1$ and the other end in $V_2$.\\
A Graph $G=(V,E)$ is bipartite if and only if it has no odd cycles.(odd cycle: a cycle with odd number of edges)\\
\noindent
\textbf{Testing bipartiteness}:\\
Given a graph $G=(V,E)$, we want to test if $G$ is bipartite.\\
Given a graph $G=(V,E)$, decide if it is 2-colourable.\\
Given a graph $G=(V,E)$, decide if it has an odd cycle.\\
\noindent
\textbf{Colouring the nodes}
It is quite familiar with BFS:\\
\begin{algorithm}[H]
\caption{Colouring the nodes}
\begin{algorithmic}[1]
\Procedure{Colouring}{$G,s$}
\State initial empty list $L$
\State initial empty list $C$
\State $L \gets {s}$
\State $C[s] \gets red$
\State $i \gets 0$
\While{$L[i] \neq \emptyset$}
\State $L_{i+1} \gets empty list$
\For{$v \in L[i]$}
\For{edges $(e)$ incident to $v$}
\If{$e$ is unexplored}
\State $w \gets$ the other end of $e$
\If {$w$ is unexplored}
\State label $e$ as a tree edge
\State add $w$ to $L_{i+1}$
\If {$i+1$ is odd}
\State $C[w] \gets green$
\Else 
\State $C[w] \gets red$
\EndIf
\Else
\State label $e$ as a cross edge
\If{$C[v] = C[w]$}
\State \Return False
\EndIf
\EndIf
\EndIf
\EndFor
\EndFor
\State $i \gets i+1$
\EndWhile
\For {$e(v,w) \in G$}
\If {$C[v] = C[w]$}
\State \Return False
\EndIf
\EndFor
\State \Return True
\EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
\textbf{Running time of colouring the nodes}: $O(n+m)$\\
\textbf{Correctness of colouring the nodes}:\\
Proof by contradiction.\\
Suppose that $G$ is not bipartite.\\
Then $G$ has an odd cycle.\\
Suppose to the contrary that the algorithm return True.\\
That means that the algorithm did not detect the odd cycle.\\

\section{DAGs and Topological Ordering}
\textbf{DAG}: A directed acyclic graph (DAG) is a directed graph with no directed cycles.\\
examples of DAGs:\\
\begin{center}
    \begin{tikzpicture}
\centering
\node[shape=circle,draw=black] (A) at (0,0) {0};
\node[shape=circle,draw=black] (B) at (2,0) {1};
\node[shape=circle,draw=black] (C) at (2,2) {2};
\node[shape=circle,draw=black] (D) at (0,2) {3};
\node[shape=circle,draw=black] (E) at (4,0) {4};
\node[shape=circle,draw=black] (F) at (4,2) {5};
\path [->] (A) edge node[left] {} (B);
\path [->] (B) edge node[left] {} (C);
\path [->] (C) edge node[left] {} (E);
\path [->] (D) edge node[left] {} (A);
\path [->] (D) edge node[left] {} (C);
\path [->] (E) edge node[left] {} (F);
\end{tikzpicture}\\
\end{center}
\noindent
\textbf{Topological ordering}:Given a graph$G=(V,E)$, a topological ordering of $G$ is an ordering of the nodes $u_1,u_2,\dots,u_n$ such that for every edge $(u_i,u_j)$, we have $i<j$.\\
Intutively, a topological ordering is an ordering of the nodes such that every edge goes from left to right.\\
example of topological ordering based on given graph above:\\
\begin{equation*}
    3,0,1,2,4,5
\end{equation*}
\noindent
\textbf{Topological ordering implies DAG}:\\
\begin{itemize}
    \item If $G$ has a topological ordering, then $G$ is a DAG.
    \item Suppose by contradiction that $G$ has a topological ordering $u_1,u_2,\dots,u_n$ but $G$ also has a cycle $C$.
    \item Let $u_j$ be the smallest element of $C$ in the topological ordering.
    \item Let $u_i$ be its predecessor in $C$.
    \item $u_i$ must appear before $u_j$ in the topological ordering.
    \item This contradicts the fact that $u_j$ is the smallest element of $C$ in the topological ordering.
\end{itemize}

\noindent
\textbf{DAG implies topological ordering}:\\
Proof by induction:
Base case: If $G$ has one or two nodes, then $G$ has a topological ordering.\\
Induction steps: Assume that a DAG up to $k$ nodes has a topological ordering(induction hypothesis). we will prove that a DAG with $k+1$ nodes has a topological ordering.
\begin{itemize}
    \item By our lemma, there is at least one source node in $G$, and let $u$ be the node.
    \item Put $u$ at the beginning of the topological ordering.
    \item Consider the graph $G'$, obtained by $G$ by removing $u$ and its incident edges.
    \item $G'$ is a DAG with $k$ nodes.
    \item It has a topological ordering $u_1,u_2,\dots,u_k$ by the induction hypothesis.
    \item Append this ordering to $u$ to get a topological ordering of $G$.
\end{itemize}
\noindent
Here is the algorithm:\\
\begin{algorithm}[H]
\caption{Topological Sorting}
\begin{algorithmic}[1]
\Procedure{TopologicalSorting}{$G$}
\State find a source vertex $u$
\State set $u$ as the first element of the topological ordering
\State $G' \gets G$ with $u$ and its incident edges removed
\State $L \gets$ \Call{TopologicalSorting}{$G'$}
\State append $L$ to $u$
\EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
Running time of the algorithm is $O(n^2)$\\
\textbf{Modified Topological Sorting}:\\
Running time of the algorithm is $O(n+m)$\\
\begin{algorithm}[H]
\caption{Modified Topological Sorting}
\begin{algorithmic}[1]
\Procedure{ModifiedTopologicalSorting}{$G$}
\State $L \gets empty list$
\State $S \gets$ set of all source vertices
\While{$S \neq \emptyset$}
\State remove a vertex $u$ from $S$
\State append $u$ to $L$
\For{each edge $(u,v)$}
\State remove edge $(u,v)$ from $G$
\If{$v$ is a source vertex}
\State add $v$ to $S$
\EndIf
\EndFor
\EndWhile
\If{$G$ has edges}
\State \Return $G$ has a cycle
\Else
\State \Return $L$
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Finding strongly connected components}
\textbf{connected components}: A connected component of an undirected graph is subgraph of the graph where any two nodes are connected by a path.\\
\textbf{strongly connected components}: A strongly connected component of a directed graph is a subgraph of the graph where any two nodes are mutually reachable.(mutually reachable: there is a path from $u$ to $v$ and a path from $v$ to $u$)\\
\textbf{Finding strongly connected components}:\\
\textbf{Kosaraju's algorithm}:\\
\begin{algorithm}[H]
\caption{Kosaraju's algorithm}
\begin{algorithmic}[1]
\Procedure{Kosaraju}{$G$}
\State Initialise stack $S$
\State Select a arbitrary node $s$
\State DFS\_tree=DFS($G,s$)
\State $S \gets$ nodes in DFS\_tree
\State $G^R \gets$ nodes in order of $S$
\State DFS($G^R,s$)
\State \Return the nodes in the DFS tree
\EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
\textbf{Running time of Kosaraju's algorithm}: $O(n+m)$\\
\textbf{Correctness of Kosaraju's algorithm}:
\begin{itemize}
    \item Define a meta-graph of $G$, called $G^{SCC}=(V^{SCC},E^{SCC})$.
    \item Supposed that $G$ has strongly connected components (SCCs) $C_1,C_2,\dots,C_k$, for some $k$.
    \item $V^{SCC} = \{C_1,C_2,\dots,C_k\}$ contains some of the SCCs of $G$.
    \item There is an edge $(C_i,C_j)$ in $E^{SCC}$ if $G$ contains a directed edge ($x,y$) such that $x \in C_i$ and $y \in C_j$, crossing different components.
\end{itemize}
\noindent
Examples:
\begin{center}
\begin{tikzpicture}
\centering
\node[shape=circle,draw=black] (A) at (0,0) {0};
\node[shape=circle,draw=black] (B) at (2,0) {1};
\node[shape=circle,draw=black] (C) at (2,2) {2};
\node[shape=circle,draw=black] (D) at (0,2) {3};
\node[shape=circle,draw=black] (E) at (4,0) {4};
\node[shape=circle,draw=black] (F) at (4,2) {5};
\path [->] (A) edge node[left] {} (B);
\path [->] (B) edge node[left] {} (C);
\path [->] (C) edge node[left] {} (E);
\path [->] (D) edge node[left] {} (A);
\path [->] (C) edge node[left] {} (D);
\path [->] (E) edge node[left] {} (F);
\end{tikzpicture}\\
\end{center}
\noindent
The SCCs are $\{0,1,2,3\}$ and $\{4,5\}$.\\
\noindent
The meta-graph is:\\
\begin{center}
\begin{tikzpicture}
\centering
\node[shape=circle,draw=black] (A) at (0,0) {$\{0,1,2,3\}$};
\node[shape=circle,draw=black] (B) at (2,0) {$\{4\}$};
\node[shape=circle,draw=black] (C) at (2,2) {$\{5\}$};
\path [->] (A) edge node[left] {} (B);
\path [->] (B) edge node[left] {} (C);
\end{tikzpicture}\\
\end{center}
\noindent


\chapter{Greedy Algorithms}
\textbf{The greedy approach}:
\begin{itemize}
    \item The goal is to find a global solution to a problem.
    \item The solution will be built up in small consecutive steps.
    \item For each step, we choose the best option available to us at that moment.
\end{itemize}
\section{Interval Scheduling}
\textbf{Interval Scheduling}:\\
A set of requests$R=\{1,2,\dots,n\}$.
\begin{itemize}
    \item Each request $i$ has a start time $s_i$ and a finish time $f_i$.
    \item Alternative view: every request is an interval $[s_i,f_i]$.
\end{itemize}
Two requests $i$ and $j$ are compatible if $[s_i,f_i]$ and $[s_j,f_j]$ do not overlap.\\
\textbf{Goal}: Find a maximum-size subset of compatible requests.\\
\textbf{Example}:\\
\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/scheduling.png}
    \caption{Interval Scheduling}
    \label{fig:my_label}
\end{figure}
\noindent
\textbf{Interval Scheduling Algorithm}:
\begin{algorithm}[H]
\caption{Interval Scheduling Algorithm}
\begin{algorithmic}[1]
\Procedure{IntervalScheduling}{$[s_1,f_1],[s_2,f_2],\dots,[s_n,f_n]$}
\State $R$ is the set of requests
\State $A \gets \emptyset$
\While{$R \neq \emptyset$}
\State select a request $i$ in $R$ with the smallest finishing time
\State add $i$ to $A$
\State remove all requests from $R$ that are incompatible with $i$
\EndWhile
\State \Return $A$
\EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
\textbf{Running time of Interval Scheduling Algorithm}: $O(n\log n)$\\
\textbf{Correctness of Interval Scheduling Algorithm}:
Since the algorithm always selects the request with the smallest finishing time, it is clear that the algorithm will always select a compatible request.\\
\textbf{Arguing optimality}:\\


\section{Minimum Spanning Trees}
Consider a connected graph $G=(V,E)$, such that each edge $e=(v,w)$ of $E$, there is an associated cost $c_e$.\\
\textbf{Goal}: Find a spanning tree $T$ of $E$ so that the graph $G'=(V,T)$ has minimum cost.\\
\textbf{Example}:\\
\begin{center}
\begin{tikzpicture}
\centering
\node[shape=circle,draw=black] (A) at (1,1) {a};
\node[shape=circle,draw=black] (B) at (2,0) {b};
\node[shape=circle,draw=black] (C) at (2,2) {c};
\node[shape=circle,draw=black] (D) at (0,2) {d};
\node[shape=circle,draw=black] (E) at (4,0) {e};
\node[shape=circle,draw=black] (F) at (4,2) {f};
\node[shape=circle,draw=black] (G) at (8,0) {g};
\node[shape=circle,draw=black] (H) at (6,1) {h};
\path [-] (A) edge node[left] {4} (B);
\path [-] (A) edge node[left] {3} (C);
\path [-] (A) edge node[left] {1} (D);
\path [-] (B) edge node[left] {2} (C);
\path [-] (B) edge node[left] {3} (E);
\path [-] (C) edge node[left] {5} (D);
\path [-] (C) edge node[left] {4} (F);
\path [-] (E) edge node[left] {4} (G);
\path [-] (E) edge node[left] {2} (F);
\path [-] (F) edge node[left] {1} (H);
\path [-] (G) edge node[left] {2} (H);
\path [-] (E) edge node[left] {4} (H);
\end{tikzpicture}\\
\end{center}

\textbf{Greedy approach 1}:\\
\begin{itemize}
    \item Start with an empty set of edges $T$.
    \item Repeat until $T$ forms a spanning tree:
    \begin{itemize}
        \item Select an edge $e$ of minimum cost.
        \item If $T \cup \{e\}$ does not contain a cycle, then add $e$ to $T$.
    \end{itemize}
\end{itemize}
\textbf{krukals algorithm}:
\begin{algorithm}[H]
\caption{Krukals algorithm}
\begin{algorithmic}[1]
\Procedure{Krukals}{$G$}
\State $T \gets \emptyset$
\While{$T$ is not a spanning tree}
\State select an edge $e$ of minimum cost
\If{$T \cup \{e\}$ does not contain a cycle}
\State add $e$ to $T$
\EndIf
\EndWhile
\State \Return $T$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\noindent
\textbf{Running time of Krukals algorithm}: $O(m\log n)$\\
\textbf{Greedy approach 2}:
\begin{itemize}
    \item Start with an empty set of edges $T$.
    \item Start with a node $s$.
    \begin{itemize}
        \item Add an edge $e=(s,v)$ of minimum cost to $T$.
    \end{itemize}
    \item Repeat until $T$ forms a spanning tree:
\end{itemize}
\textbf{Prims algorithm}:
\begin{algorithm}[H]
\caption{Prims algorithm}
\begin{algorithmic}[1]
\Procedure{Prims}{$G$}
\State $T \gets \emptyset$
\State $s \gets$ an arbitrary node
\While{$T$ is not a spanning tree}
\State add an edge $e=(s,v)$ of minimum cost to $T$
\State $s \gets v$
\EndWhile
\State \Return $T$
\EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
\textbf{Running time of Prims algorithm}: $O(m\log n)$\\
\textbf{minimum spanning tree of example graph}:\\
the minimum spanning tree sequence is $d,a,c,b,e,f,h,g$.\\
\noindent

\textbf{Greedy approach 3}:\\
\begin{itemize}
    \item Start with the full graph $G=(V,E)$.
    \item Delete an edge from $G$
    \begin{itemize}
        \item the edge of maximum cost
    \end{itemize}
    \item Repeat until $G$ forms a spanning tree:
\end{itemize}
\textbf{Reverse-delete algorithm}:
\begin{algorithm}[H]
\caption{Reverse-delete algorithm}
\begin{algorithmic}[1]
\Procedure{ReverseDelete}{$G$}
\State $T \gets G$
\While{$T$ is not a spanning tree}
\State delete an edge $e$ of maximum cost from $T$
\EndWhile
\State \Return $T$
\EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
For when two edges have the same cost, use distinct labels to distinguish them.\\

\noindent
\textbf{Optimal with Priorty Queue}:\\
Add PQ to Prim's algorithm.\\
\begin{algorithm}[H]
\caption{Optimal with Priorty Queue}
\begin{algorithmic}[1]
\Procedure{Optimal}{$G$}
\State $T \gets \emptyset$
\State $s \gets$ an arbitrary node
\State $PQ \gets$ empty priority queue
\For{each node $v$}
\State add $v$ to $PQ$ with key $\infty$
\EndFor
\State decrease key of $s$ to 0
\While{$PQ$ is not empty}
\State $v \gets$ node with minimum key in $PQ$
\State add an edge $e=(s,v)$ of minimum cost to $T$
\State $s \gets v$
\For{each edge $e=(v,w)$ incident to $v$}
\If{$w$ is in $PQ$}
\State decrease key of $w$ to $c_e$
\EndIf
\EndFor
\EndWhile
\State \Return $T$
\EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
\textbf{Running time of Optimal with Priorty Queue}: $O(m\log n)$\\

\section{Clustering}
\begin{itemize}
    \item a collection of $n$ objects
    \item they have different degrees of similarity
    \item we want to organise them into coherent groups
    \item there is a notion of distance between objects
\end{itemize}
\noindent
\textbf{Definition}:
\begin{itemize}
    \item Given a set $U$ of $n$ elements, a $k-$clustering of $U$ is a partition of $U$ into non-empty subsets $C_1,C_2,\dots,C_k$.
    \item The spacing of a $k-$clustering is the minimum distance between any pair of points in different clusters.
\end{itemize}
\textbf{Goal}:Among all possible $k-$clusterings, find one with minimum spacing.\\
\textbf{Example}:\\
\begin{center}
\begin{tikzpicture}
\centering
\node[shape=circle,draw=black] (A) at (1,1) {a};
\node[shape=circle,draw=black] (B) at (2,0) {b};
\node[shape=circle,draw=black] (C) at (2,2) {c};
\node[shape=circle,draw=black] (D) at (0,2) {d};
\node[shape=circle,draw=black] (E) at (4,0) {e};
\node[shape=circle,draw=black] (F) at (4,2) {f};
\node[shape=circle,draw=black] (G) at (8,0) {g};
\node[shape=circle,draw=black] (H) at (6,1) {h};
\path [-] (A) edge node[left] {4} (B);
\path [-] (A) edge node[left] {3} (C);
\path [-] (A) edge node[left] {1} (D);
\path [-] (B) edge node[left] {2} (C);
\path [-] (B) edge node[left] {3} (E);
\path [-] (C) edge node[left] {5} (D);
\path [-] (C) edge node[left] {4} (F);
\path [-] (E) edge node[left] {4} (G);
\path [-] (E) edge node[left] {2} (F);
\path [-] (F) edge node[left] {1} (H);
\path [-] (G) edge node[left] {2} (H);
\path [-] (E) edge node[left] {4} (H);
\end{tikzpicture}\\
\end{center}
\textbf{Greedy approach}:\\
\begin{itemize}
    \item Pick two objects $p_i$ and $p_j$ with minimum distance $d(p_i,p_j)$.
    \item Connect them with an edge $e=(p_i,p_j)$.
    \item Continue like this until we have $k$ clusters.
    \item If the edge $e$ under consideration connects two object $p_i$ and $p_j$ already in the same cluster, then discard $e$.
\end{itemize}
\textbf{kruskals algorithm}:
\begin{algorithm}[H]
\caption{kruskals algorithm for clustering}
\begin{algorithmic}[1]
    \Require A graph $G = (V,E)$
    \Ensure A minimum spanning tree of $G$ with $k$ clusters
    
    \Procedure{Kruskal}{$G,k$}
      \State $T \gets \emptyset$
      \State $C \gets \{ \{v\} \mid v \in V \}$ \Comment{Initial clusters}
      \State Sort edges in $E$ in increasing order of weight
      \For{$\{u,v\} \in E$}
        \If{$C$ contains $k$ clusters}
          \State \textbf{break}
        \EndIf
        \If{clusters containing $u$ and $v$ are different in $C$}
          \State $T \gets T \cup \{\{u,v\}\}$
          \State merge clusters containing $u$ and $v$ in $C$
        \EndIf
      \EndFor
      \State \textbf{return} $T$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
For Given example, the result of divide them into 3 clusters is:\\
\begin{equation*}
    \{a,b,c,d\},\{e,f,h\},\{g\}
\end{equation*}
\noindent

\chapter{Dynamic Programming}
\textbf{The paradigm of dynamic programming}:
Given a problem $P$, define a sequence of subproblems, with the following properties:
\begin{itemize}
    \item The subproblems are ordered from the simplest to the largest
    \item The largest problem is our original problem $P$
    \item The optimal solution of a subproblem can be structured from the optimal solutions of smaller subproblems.
\end{itemize}
Solve the subproblems from the smallest to the largest. When you solve a subproblem, store the solution and use it to solve larger subproblems.\\
\section{Weighted Interval Scheduling}
\begin{itemize}
    \item A set of requests $R=\{1,2,\dots,n\}$.
    \begin{itemize}
        \item Request $i$ has a start time $s_i$ and a finish time $f_i$, and a value $v_i$.
        \item Alternative view: every request is an interval $[s_i,f_i]$ associated with a value $v_i$.
    \end{itemize}
    \item Two requests $i$ and $j$ are compatible if $[s_i,f_i]$ and $[s_j,f_j]$ do not overlap.
\end{itemize}
\textbf{build up a solution}:
\begin{enumerate}
    \item let $O$ the optimal solution
    \item $O$ contains an optimal solution $O'$ of the subproblem $R'=\{1,2,\dots,i-1\}$
    \item in order to find $O$, it suffices to look at smaller problems and find $O(1,2,\dots,j)$ for some $j$
    \item Let $O_j$ be a shorthand for $O(1,2,\dots,j)$ and let $OPT(j)$ be its total value.
    \item Define $OPT(0)=0$
    \item Then $O=O_n$ with value $OPT(n)$
    \item $OPT(j)$ can be computed from $OPT(j-1)$
    \item $OPT(j)=\max\{OPT_{p_j}+v_j,OPT(j-1)\}$
\end{enumerate}

\begin{algorithm}[H]
\caption{ComputeOPT}
\begin{algorithmic}[1]
    \Procedure{ComputeOPT}{$j$}
        \If{$j=0$}
            \State \textbf{return} $0$
        \Else
            \State \textbf{return} $\max\{\textsc{ComputeOPT}(p_j)+v(j),\textsc{ComputeOPT}(j-1)\}$
        \EndIf
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
\textbf{Correctness}:
ComputeOPT(j) correctly computes $OPT(j)$ for all $j=0,1,\dots,n$.\\
Proof by induction:\\
\textbf{Base case}: $OPT(0)=0$ by definition.\\
\textbf{Inductive step}: Assume that it is true for all $i<j$.(Induction hypothesis)\\
return $\max\{\textsc{ComputeOPT}(p_j)+v(j),\textsc{ComputeOPT}(j-1)\}$\\
\noindent
\textbf{Running time}: $\Omega(2^n)$\\
\textbf{Memoization}:
\begin{itemize}
    \item Compute ComputeOPT(j) for all $j=0,1,\dots,n$.
    \item Store it in an accessible place to use again later.
    \item Keep an array $M[0,\dots,n]$.
    \begin{itemize}
        \item initially $M[j]=\textsc{empty}$ for all $j=0,1,\dots,n$.
        \item when ComputeOPT(j) is called, $M[j]=$ ComputeOPT(j).
    \end{itemize}
\end{itemize}
\begin{algorithm}[H]
\caption{M-ComputeOPT}
\begin{algorithmic}
    \Procedure{M-ComputeOPT}{$j$}
        \If {$j=0$}
            \State \textbf{return} $0$
        \ElsIf {$M[j]$ is not empty}
            \State \textbf{return} $M[j]$
        \Else
            \State $M[j] \gets \max\{\textsc{M-ComputeOPT}(p_j)+v(j),\textsc{M-ComputeOPT}(j-1)\}$
            \State \textbf{return} $M[j]$
        \EndIf
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
\textbf{Running time}: $O(n \log n)$\\
\begin{algorithm}[H]
\caption{Find-Solution}
\begin{algorithmic}
    \Procedure{Find-Solution}{$j$}
        \If {$j=0$}
            \State \textbf{return} $\emptyset$
        \Else 
            \If $v(j)+\textsc{M-ComputeOPT}(p_j)>\textsc{M-ComputeOPT}(j-1)$
                \State \textbf{return} $\{j\} \cup \textsc{Find-Solution}(p_j)$
            \Else
                \State \textbf{return} $\textsc{Find-Solution}(j-1)$
            \EndIf
        \EndIf
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
\clearpage
\noindent
\textbf{Dynamic Programming vs Divide and Conquer}:
\begin{multicols}{2}
\noindent
\textbf{Dynamic Programming}:
\begin{itemize}
    \item DP is an optimisation techniques and is only applicable to problems that have optimal substructure.
    \item DP splits the problem into parts, finds solutions to the parts and joins them.(The parts are not significantly smaller than the original problem and are overlapping.)
    \item In DP, the subproblems dependency can be represented by a directed acyclic graph.
\end{itemize}
\columnbreak
\textbf{Divide and Conquer}:
\begin{itemize}
    \item DC is not normally used for optimisation problems.
    \item DC splits the problem into parts, finds solutions to the parts and joins them.(The parts are significantly smaller than the original problem and are non-overlapping.)
    \item In DC, the subproblems dependency can be represented by a tree.
\end{itemize}
\end{multicols}
\noindent
\section{Subset Sum}
\textbf{Problem Description}:
\begin{itemize}
    \item Given a set of $n$ items ${1,2,\dots,n}$
    \item Each item $i$ has a non-negative weight $w_i$.
    \item Given a bound $W$.
    \item \textbf{Goal}: select a subset $S$ of items such that $\sum_{i\in S}w_i\leq W$ and $\sum_{i\in S}w_i$ is maximised.
\end{itemize}
Dynamic Programming:
To find the optimal value of $OPT(n)$, we need
\begin{itemize}
    \item the optimal value of $OPT(n-1)$ if item $n$ is not selected.
    \item the optimal value of the solution on input {1,2,\dots,n-1} with weight bound $W-w_n$.
\end{itemize}
subproblems:
\begin{itemize}
    \item Assumptions:
    \begin{itemize}
        \item $W$ is an integer
        \item Every $w_i$ is an integer
    \end{itemize}
    \item subproblem for each $i=0,1,\dots,n$ and each integer $0 \leq w \leq W$.
    \item Let $OPT(i,w)$ be the optimal value of the solution on subset ${1,2,\dots,i}$ with weight bound $w$.
\end{itemize}
\begin{algorithm}[H]
\caption{SubsetSum}
\begin{algorithmic}
    \Procedure{SubsetSum}{$n,w$}
        \State Array $M[0,\dots,n,0,\dots,W]$
        \State $M[0,w]=0$ for each $w=0,1,\dots,W$
        \For {$i=1$ to $n$}
            \For {$w=0$ to $W$}
                \If {$w_i>w$}
                    \State $M[i,w]=M[i-1,w]$
                \Else
                    \State $M[i,w]=\max\{M[i-1,w],M[i-1,w-w_i]+w_i\}$
                \EndIf
            \EndFor
        \EndFor
        \State \textbf{return} $M[n,W]$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
\textbf{Running time}: $O(nW)$\\


\section{knapSack}
\textbf{Problem Description}:
\begin{itemize}
    \item Given a set of $n$ items ${1,2,\dots,n}$
    \item Each item $i$ has a non-negative weight $w_i$ and a non-negative value $v_i$.
    \item Given a bound $W$.
    \item \textbf{Goal}: select a subset $S$ of items such that $\sum_{i\in S}w_i\leq W$ and $\sum_{i\in S}v_i$ is maximised.
\end{itemize}
\textbf{the fractional knapsack problem}:
\begin{itemize}
    \item Given a set of $n$ items ${1,2,\dots,n}$
    \item Each item $i$ has a non-negative weight $w_i$ and a non-negative value $v_i$.
    \item Given a bound $W$.
    \item \textbf{Goal}: select a fraction $x_i$ of each item $i$ such that $\sum_{i\in S}w_ix_i\leq W$ and $\sum_{i\in S}v_ix_i$ is maximised.
\end{itemize}
\textbf{The 0/1 knapsack problem}:
Solution for 0/1 knapsack problem:
\begin{algorithm}
\caption{0/1 knapsack in dynamic programming}
\begin{algorithmic}
    \Procedure{0/1 knapsack}{$n,W$}
        \State Array $M[0,\dots,n,0,\dots,W]$
        \State $M[0,w]=0$ for each $w=0,1,\dots,W$
        \For {$i=1$ to $n$}
            \For {$w=0$ to $W$}
                \If {$w_i>w$}
                    \State $M[i,w]=M[i-1,w]$
                \Else
                    \State $M[i,w]=\max\{M[i-1,w],M[i-1,w-w_i]+v_i\}$
                \EndIf
            \EndFor
        \EndFor
        \State \textbf{return} $M[n,W]$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

\chapter{Network Flow}
\section{Network Flow Definitions}
\textbf{Flow network}:
A flow network is a directed graph $G=(V,E)$ with the following properties:
\begin{itemize}
    \item Each edge $(u,v)\in E$ has a non-negative capacity $c_e$.
    \item There is a single source $s$ in $V$.
    \item There is a single sink $t$ in $V$.
    \item All other nodes in $V-\{s,t\}$ are called intermediate nodes.
\end{itemize}
example:
\begin{center}
\begin{tikzpicture}
    \node[shape=circle,draw=black] (s) at (0,0) {$s$};
    \node[shape=circle,draw=black] (a) at (2,2) {$a$};
    \node[shape=circle,draw=black] (b) at (2,-2) {$b$};
    \node[shape=circle,draw=black] (c) at (4,2) {$c$};
    \node[shape=circle,draw=black] (d) at (4,-2) {$d$};
    \node[shape=circle,draw=black] (t) at (6,0) {$t$};
    \path [->] (s) edge node[above] {$10$} (a);
    \path [->] (s) edge node[below] {$5$} (b);
    \path [->] (a) edge node[above] {$4$} (c);
    \path [->] (a) edge node[left] {$8$} (d);
    \path [->] (b) edge node[below] {$3$} (c);
    \path [->] (b) edge node[below] {$2$} (d);
    \path [->] (c) edge node[above] {$9$} (t);
    \path [->] (d) edge node[below] {$7$} (t);
\end{tikzpicture}
\end{center}
\noindent
\textbf{Further definitions}:
\begin{itemize}
    \item The source $s$ has no incoming edges.
    \item The sink $t$ has no outgoing edges.
    \item There is at least one edge incident to each node.
    \item All capacities are integers.
\end{itemize}
\noindent
\textbf{Flow}:
An $(s-t)$ flow is a function $f:E \rightarrow \mathbb{R^+}$, mapping each edge $e$ to a non-negative real number $f(e)$.\\
A feasible flow must satisfy the following conditions:
\begin{itemize}
    \item Capacity: For each edge $e\in E$, $0 \leq f(e) \leq c_e$.
    \item Flow conservation: for each node $v\in V-\{s,t\}$, we have
    \begin{equation*}
        \sum_{e \:\text{into} \: v}f(e)=\sum_{e \: \text{out of}\: v}f(e)
    \end{equation*}
\end{itemize}
The source $s$ generates flow, and the sink $t$ absorbs flow.\\
Value of a flow $f$, denoted $val(f)$, is the total amount of flow generated by the source $s$:
\begin{equation*}
    v(f)=\sum_{e \: \text{out of}\: s}f(e)
\end{equation*}
Generally, define $f^{out}(v)$ and $f^{in}(v)$ for the flow going out of(resp. going into) node $v$.\\
Similarly, define $f^{out}(S)$ and $f^{in}(S)$ for sets of nodes $S$.\\

\noindent
\section{Maximum Flow Problem}
\textbf{The maximum flow problem}: Given a flow network $G=(V,E)$, find a flow of maximum possible value.\\
\textbf{algorithm for maximum flow}:\\
Idea: push flow forward on edges with leftover capacity, push flow backward on edges that are already carrying flow.\\
\noindent
\textbf{The residual graph $G_f$}:\\
The residual graph $G_f$ of $G$(also called the flow network) is defined as follows:
\begin{itemize}
    \item The node set  $V_f$ of $G_f$ is the same as the node set $V$.
    \item For each edge $(u,v)\in E$ which $f(e) < c_e$, there are $c_e-f(e)$ "leftover" units of capacity.
    \begin{itemize}
        \item We will call this number the \textbf{residual capacity} of edge $e$.
        \item We will call the edge $e$ a forward edge.
    \end{itemize}
    \item For each edge $(u,v)\in E$ with $f(e)>0$, there is an edge $e'= (v,u)$ in $E_f$ with a capacity of $f(e)$.We will call the edge $e'$ a backward edge.
\end{itemize}

\textbf{Working with residual graphs}:
\begin{itemize}
    \item Find an $(s-t)$ path $P$ in $G_f$. This is called an \textbf{augmenting path}.
    \item Define the bottleneck of $P$,
    \begin{itemize}
        \item Denoted bottleneck$(P,f)$
        \item to be the minimum residual capacity of any edge in $P$.
    \end{itemize}
    \item Define the augmentation of flow $f$ into flow $f'$
    \begin{itemize}
        \item Denoted augment$(f,P)$.
    \end{itemize}
\end{itemize}

\textbf{Augmenting the flow}:\\
\begin{algorithm}
\caption{Augmenting the flow}
\begin{algorithmic}[1]
    \Procedure{Augment}{$f,P$}
        \State $b \gets \text{bottleneck}(P,f)$
        \For{each edge $e=(u,v)\in P$}
            \If{$e$ is a forward edge}
                \State $f(e) \gets f(e)+b$
            \Else
                \State $f(e) \gets f(e)-b$
            \EndIf
        \EndFor
        \State \textbf{return} $f$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
Feasibility of capacity:\\
consider an arbitrary edge $e=(u,v)\in P$. Suppose that $e$ is a forward edge.
\begin{equation*}
    0 \leq f(e) \leq f'(e) = f(e)+b \leq f(e)+(c_e-f(e))=c_e
\end{equation*}
Suppose that $e$ is a backward edge.
\begin{equation*}
    c_e \geq f(e) \geq f'(e) = f(e)-b \geq f(e)-(f(e)-0)=0
\end{equation*}
\noindent
\textbf{The Ford-Fulkerson algorithm}:\\
\begin{algorithm}[H]
\caption{Max-flow algorithm}
\begin{algorithmic}[1]
    \Procedure{Max-flow}{$G,s,t$}
        \State $f(e) \gets 0$ for all edges $e\in E$
        \While{there exists an $(s-t)$ path $P$ in $G_f$}
            \State $f \gets \text{augment}(f,P)$
            \State $f' \gets \text{update} (f)$
            \State $G_f \gets \text{update}(G_f,f)$
        \EndWhile
        \State \textbf{return} $f$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
\textbf{Running time of Ford-Fulkerson algorithm}: $O(mC)$, where $C$ is the maximum capacity of any edge in the network.\\


\section{Min Cut theorem}
A cut $C$ is a partition of the nodes of $G$ into two sets $S$ and $T$ such that $s\in S$ and $t\in T$.\\
The capacity of a cut $C=(S,T)$ of a cut $C$ is the sum of the capacities of the edges "out of" $S$: these are edges $(u,v)$ such that $u\in S$ and $v\in T$.\\

\noindent
\textbf{The min-cut theorem}: In every flow network, the value of the maximum flow is equal to the capacity of the minimum cut.\\
\noindent
\textbf{A series of facts}:\\
Fact 1: Let $f$ be any $(s-t)$ flow and let $(S,T)$ be any cut. Then $v(f)=f^{out}(S)-f^{in}(S)$.\\
\begin{enumerate}
    \item By definition, $v(f)=f^{out}(s)$.
    \item By definition $f^{in}(s)=0$.
    \item Hence, $v(f)=f^{out}(s)-f^{in}(s)$.
    \item For every other node $v \neq s,t$, we have $f^{out}(v)=f^{in}(v)$.
    \item Therefore, $v(f)=\sum_{v\in S}(f^{out}(v)-f^{in}(v))$.
    \item rewrite as $v(f)=\sum_{v\in S}(f^{out}(v)-f^{in}(v))=\sum_{e \: \text{out of} \: S}f(e)-\sum_{e \: \text{into} \: S}f(e)=f^{out}(S)-f^{in}(S)$.
\end{enumerate}
Fact 2: Let $f$ be any $(s-t)$ flow and let $(S,T)$ be any $(s-t)$ cut. Then $v(f)=f^{out}(T)-f^{in}(T)$.\\
Fact 3: Let $f$ be any $(s-t)$ flow and let $(S,T)$ be any $(s-t)$ cut. Then $v(f) \leq c(S,T)$.\\
\begin{align*}
    v(f) &= f^{out}(S)-f^{in}(S)\\
    &\leq f^{out}(S)\\
    &= \sum_{e \: \text{out of} \: S}f(e)\\
    &\leq \sum_{e \: \text{out of} \: S}c(e)\\
    &= c(S,T)
\end{align*}
Fact 4: Let $f$ be any $(s-t)$ flow in $G$ such that the residual graph $G_f$ contains no augmenting paths. Then there exists an $(s-t)$ cut $(S^*,T^*)$ such that $v(f)=c(S^*,T^*)$.\\
Proving fact 4: In the residual graph $G_f$, identify all nodes that are reachable from the source $s$. Let $S^*$ be the set of these nodes, and let $T^*=V-S^*$.
\begin{enumerate}
    \item $s\in S^*$ and $t\in T^*$.
    \item No edge of $G_f$ crosses from $S^*$ to $T^*$.
    \item Every edge of $G_f$ crosses from $T^*$ to $S^*$.
    \item $f^{out}(S^*)=v(f)$.
    \item $f^{in}(S^*)=0$.
    \begin{align*}
        v(f) &= f^{out}(S^*)-f^{in}(S^*)\\
        &= \sum_{e \: \text{out of} \: S^*}f(e)-\sum_{e \: \text{into} \: S^*}f(e)\\
        &= \sum_{e \: \text{out of} \: S^*}c(e)-0
        &= c(S^*,T^*)
    \end{align*}
\end{enumerate}
Fact 5: If all capacities are integers, then there exists a maximum flow $f$ for which $f(e)$ is an integer for every edge $e$.\\


\section{Choosing Better Augmenting Paths}
\textbf{The Edmonds-Karp algorithm}:\\
\begin{algorithm}[H]
\caption{Edmonds-Karp algorithm}
\begin{algorithmic}[1]
    \Procedure{Edmonds-Karp}{$G,s,t$}
        \State $f(e) \gets 0$ for all edges $e\in E$
        \While{there exists an $(s-t)$ path $P$ in $G_f$}
            \State $P$ is a shortest $(s-t)$ path
            \State $f \gets \text{augment}(f,P)$
            \State $f' \gets \text{update} (f)$
            \State $G_f \gets \text{update}(G_f,f)$
        \EndWhile
        \State \textbf{return} $f$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
\textbf{Running time of Edmonds-Karp algorithm}: $O(nm^2)$, where $n$ is the number of nodes and $m$ is the number of edges in the network.\\
The shortest path can be found in $O(m)$ time using BFS.\\

\section{Modeling with Network Flows}
\textbf{Bipartite graphs}: A graph $G=(V,E)$ is bipartite if any only if it can be partitioned into two sets $A$ and $B$ such that every edge has one endpoint in $A$ and one endpoint in $B$.\\
\textbf{Maximum bipartite matching}:\\
Matching: A subset $M$ of edges $E$ such that each node $v \in V$ appears in at most one edge $e \in M$.\\
Maximum matching: A matching with maximum cardinality.\\
\noindent
examples of bipartite graphs:\\
\begin{center}
\begin{tikzpicture}
    \node[shape=circle,draw=black] (A) at (0,0) {A};
    \node[shape=circle,draw=black] (B) at (0,-1) {B};
    \node[shape=circle,draw=black] (C) at (0,-2) {C};
    \node[shape=circle,draw=black] (D) at (0,-3) {D};
    \node[shape=circle,draw=black] (E) at (0,-4) {E};
    \node[shape=circle,draw=black] (H) at (4,0) {H};
    \node[shape=circle,draw=black] (I) at (4,-1) {I};
    \node[shape=circle,draw=black] (J) at (4,-2) {J};
    \node[shape=circle,draw=black] (K) at (4,-3) {K};
    \node[shape=circle,draw=black] (L) at (4,-4) {L};
    \draw (A) -- (H);
    \draw (B) -- (H);
    \draw (B) -- (I);
    \draw (B) -- (J);
    \draw (C) -- (J);
    \draw (C) -- (K);
    \draw (C) -- (L);
    \draw (D) -- (L);
    \draw (E) -- (L);
\end{tikzpicture}
\end{center}
\noindent
\begin{multicols}{2}
example of maximum bipartite matching:\\
\begin{center}
\begin{tikzpicture}
    \node[shape=circle,draw=black] (A) at (0,0) {A};
    \node[shape=circle,draw=black] (B) at (0,-1) {B};
    \node[shape=circle,draw=black] (C) at (0,-2) {C};
    \node[shape=circle,draw=black] (D) at (0,-3) {D};
    \node[shape=circle,draw=black] (E) at (0,-4) {E};
    \node[shape=circle,draw=black] (H) at (4,0) {H};
    \node[shape=circle,draw=black] (I) at (4,-1) {I};
    \node[shape=circle,draw=black] (J) at (4,-2) {J};
    \node[shape=circle,draw=black] (K) at (4,-3) {K};
    \node[shape=circle,draw=black] (L) at (4,-4) {L};
    \draw (A) -- (H);
    \draw (B) -- (I);
    \draw (C) -- (J);
    \draw (E) -- (L);
\end{tikzpicture}
\end{center}
\columnbreak
exapmle of maximal bipartite matching:\\
\begin{center}
\begin{tikzpicture}
    \node[shape=circle,draw=black] (A) at (0,0) {A};
    \node[shape=circle,draw=black] (B) at (0,-1) {B};
    \node[shape=circle,draw=black] (C) at (0,-2) {C};
    \node[shape=circle,draw=black] (D) at (0,-3) {D};
    \node[shape=circle,draw=black] (E) at (0,-4) {E};
    \node[shape=circle,draw=black] (H) at (4,0) {H};
    \node[shape=circle,draw=black] (I) at (4,-1) {I};
    \node[shape=circle,draw=black] (J) at (4,-2) {J};
    \node[shape=circle,draw=black] (K) at (4,-3) {K};
    \node[shape=circle,draw=black] (L) at (4,-4) {L};
    \draw (A) -- (H);
    \draw (B) -- (H);
    \draw (B) -- (I);
    \draw (B) -- (J);
    \draw (C) -- (J);
    \draw (C) -- (K);
    \draw (C) -- (L);
    \draw (D) -- (L);
    \draw (E) -- (L);
    \draw[red] (A) -- (H);
    \draw[red] (B) -- (I);
    \draw[red] (C) -- (J);
    \draw[red] (E) -- (L);
\end{tikzpicture}
\end{center}
\end{multicols}
\noindent
From matchings to flows:\\
\begin{center}
\begin{tikzpicture}
    \node[shape=circle,draw=black] (A) at (0,0) {A};
    \node[shape=circle,draw=black] (B) at (0,-1) {B};
    \node[shape=circle,draw=black] (C) at (0,-2) {C};
    \node[shape=circle,draw=black] (D) at (0,-3) {D};
    \node[shape=circle,draw=black] (E) at (0,-4) {E};
    \node[shape=circle,draw=black] (H) at (4,0) {H};
    \node[shape=circle,draw=black] (I) at (4,-1) {I};
    \node[shape=circle,draw=black] (J) at (4,-2) {J};
    \node[shape=circle,draw=black] (K) at (4,-3) {K};
    \node[shape=circle,draw=black] (L) at (4,-4) {L};
    \draw (A) -- (H);
    \draw (B) -- (H);
    \draw (B) -- (I);
    \draw (B) -- (J);
    \draw (C) -- (J);
    \draw (C) -- (K);
    \draw (C) -- (L);
    \draw (D) -- (L);
    \draw (E) -- (L);
    \draw[red] (A) -- (H);
    \draw[red] (B) -- (I);
    \draw[red] (C) -- (J);
    \draw[red] (E) -- (L);
    \draw[->,red] (A) -- (H) node[midway,above] {1};
    \draw[->,red] (B) -- (I) node[midway,above] {1};
    \draw[->,red] (C) -- (J) node[midway,above] {1};
    \draw[->,red] (E) -- (L) node[midway,above] {1};
\end{tikzpicture}
\end{center}
\noindent
\begin{center}
\begin{tikzpicture}
    \node[shape=circle,draw=black] (A) at (0,0) {A};
    \node[shape=circle,draw=black] (B) at (0,-1) {B};
    \node[shape=circle,draw=black] (C) at (0,-2) {C};
    \node[shape=circle,draw=black] (D) at (0,-3) {D};
    \node[shape=circle,draw=black] (E) at (0,-4) {E};
    \node[shape=circle,draw=black] (H) at (4,0) {H};
    \node[shape=circle,draw=black] (I) at (4,-1) {I};
    \node[shape=circle,draw=black] (J) at (4,-2) {J};
    \node[shape=circle,draw=black] (K) at (4,-3) {K};
    \node[shape=circle,draw=black] (L) at (4,-4) {L};
    \draw (A) -- (H);
    \draw (B) -- (H);
    \draw (B) -- (I);
    \draw (B) -- (J);
    \draw (C) -- (J);
    \draw (C) -- (K);
    \draw (C) -- (L);
    \draw (D) -- (L);
    \draw (E) -- (L);
    \draw[red] (A) -- (H);
    \draw[red] (B) -- (I);
    \draw[red] (C) -- (J);
    \draw[red] (E) -- (L);
    \draw[->,red] (A) -- (H) node[midway,above] {1};
    \draw[->,red] (B) -- (I) node[midway,above] {1};
    \draw[->,red] (C) -- (J) node[midway,above] {1};
    \draw[->,red] (E) -- (L) node[midway,above] {1};
    \draw[->,blue] (A) -- (H) node[midway,above] {1};
    \draw[->,blue] (B) -- (J) node[midway,above] {1};
    \draw[->,blue] (C) -- (K) node[midway,above] {1};
    \draw[->,blue] (E) -- (L) node[midway,above] {1};
\end{tikzpicture}
\end{center}
\noindent
\textbf{Maximum Flow and Maximum Matching}\\
The size of the maximum matching is equal to the value of the maximum flow.\\
The edges of $M$ are the edges that carry flow form $A$ to $B$ in the residual network.\\
Running time: $O(mn)$\\
\textbf{Baseball Elimination}
\begin{itemize}
    \item Given a set $S$ of teams
    \item For each team $x$ in $S$, the current number of wins $w_x$
    \item For teams $x$ and $y$ in $S$, they still have to play $g_{xy}$ games against each other
    \item Given a designated team $z$
    \item Can $z$ still win the turnament?
\end{itemize}
\textbf{From Baseball Elimination to flows}
\begin{itemize}
    \item For each pair of teams $x$ and $y$, create a vertex $v_{xy}$
    \item For each team $x$, create a vertex $v_x$
    \item For each pair of teams $x$ and $y$, create an edge $(s, v_{xy})$ with capacity $g_{xy}$
    \item For each team $x$, create an edge $(v_x, t)$ with capacity $w_z + g_{xz} - w_x$
    \item For each pair of teams $x$ and $y$, create an edge $(v_{xy}, v_x)$ with infinite capacity
    \item For each pair of teams $x$ and $y$, create an edge $(v_{xy}, v_y)$ with infinite capacity
\end{itemize}
\noindent
\textbf{Open pit mining}
\begin{itemize}
    \item Given a set $S$ of blocks
    \item For each block $x$ in $S$, the value $v_x$ of the ore in the block
    \item For each block $x$ in $S$, the cost $c_x$ of mining the block
    \item For each block $x$ in $S$, the set $N_x$ of blocks that are neighbors of $x$
    \item Given a designated block $z$
    \item What is the maximum value of ore that can be mined?
\end{itemize}
\textbf{From open pit mining to flows}
\begin{itemize}
    \item For each block $x$ in $S$, create a vertex $v_x$
    \item For each block $x$ in $S$, create an edge $(s, v_x)$ with capacity $v_x$
    \item For each block $x$ in $S$, create an edge $(v_x, t)$ with capacity $c_x$
    \item For each block $x$ in $S$, create an edge $(v_x, v_y)$ with infinite capacity for each block $y$ in $N_x$
\end{itemize}

\chapter{NP-Completeness}
\section{NP-Completeness}
\textbf{Polynomial time reduction}
\begin{itemize}
    \item Given a problem $A$ to solve
    \item Reduce solving $A$ to solving $B$
    \item Assume there is an algorithm $ALG^B$ that solves $B$ at cost $O(1)$
    \item Construct an algorithm $ALG^A$ that solves $A$, which uses $ALG^B$ as a subroutine
    \item If $ALG^A$ runs in polynomial time, then this is a polynomial time reduction
\end{itemize}
\textbf{How to work with reductions}\\
Positive: Assume that I want to solve problem $A$ and I know how to solve problem $B$.\\
\indent I can try come up with a polynomial time reduction $A \leq^p B$, which will give me a polynomial time algorithm for $A$.\\
Contrapositive: Assume that there is a problem $A$ for which it is unlikely that there is a polynomial time algorithm that solves $A$.\\
\indent If I come up with a polynomial time reduction $A \leq^p B$, it is also unlikely that there is a polynomial time algorithm that solves $B$.\\
\indent $B$ is "at least as hard to solve as" $A$, because if I could solve $B$, I could also solve $A$.\\ 
\textbf{Types of reductions}
\begin{itemize}
    \item Turing reduction: $A \leq_T B$
    \begin{itemize}
        \item A reduction which solves $A$ using (potentially many) calls to an oracle for $B$
        \item As known as Cook reduction
    \end{itemize}
    \item Many-one reduction: $A \leq_m B$
    \begin{itemize}
        \item A reduction which converts instances of $A$ to instances of $B$
        \item Also known as Karp reduction
    \end{itemize}
\end{itemize}
\textbf{Problem classification}\\
Problems in $P$:\\
\indent Searching, sorting, minimum spanning tree, graph traversal, maximum flow, minimum cut, weighted interval scheduling, etc.\\
Problems in $NP$:\\
\indent subset sum, knapSack, weighted interval scheduling, Searching, sorting, minimum spanning tree, graph traversal, maximum flow, minimum cut, etc.\\
\textbf{NP-hardness}\\
A problem $B$ is NP-hard if for every problem $A$ in $NP$, $A \leq^p B$.\\
\indent If every problem in $NP$ is polynomial time reducible to $B$, then this captures the fact that $B$ is at least as hard as any problem in $NP$.\\
\textbf{3 SAT}
\begin{itemize}
    \item A CNF formula with $m$ clauses and $k$ literals.
    \begin{equation*}
        \varphi = (x_1 \lor \lnot x_5 \lor x_3) \land ( x_2 \lor x_6 \lor \lnot x_5) \land \dots \land (x_3 \lor x_8 \lor x_{12})
    \end{equation*}
    \item each clause has exactly three literals
    \item Truth assignment: A value in $\{0, 1\}$ for each variable $x_i$
    \item Satisfying assignment: A truth assignment which makes the formula evaluate to $1$
    \item Computational problem 3 SAT: Decide if the input formula $\varphi$ has a satisfying assignment.
\end{itemize}
\textbf{3 SAT is NP-complete}
\begin{itemize}
    \item 3 SAT is in $NP$
    \begin{itemize}
        \item Given a truth assignment, we can check in polynomial time if it is satisfying
    \end{itemize}
    \item 3 SAT is NP-hard
    \begin{itemize}
        \item Given a CNF formula $\varphi$, we can construct a polynomial time reduction to 3 SAT
    \end{itemize}
\end{itemize}
\textbf{Proving NP-completeness}
\indent Suppose that you are given a problem $A$ and you want to prove that $A$ is NP-complete.\\
\indent First, prove that $A$ is in $NP$.\\
\indent \indent Usually by observing that a solution is efficiently verifiable.\\
\indent Then prove that $A$ is NP-hard.\\
\indent \indent construct a polynomial time reduction from a known NP-complete problem $P$.\\

\section{NP-completeness of the vertex cover problem}
\textbf{Vertex cover}
Definition: A vertex cover $C$ of a graph $G = (V, E)$ is a subset of vertices $C \subseteq V$ such that for every edge $e$ in $E$, at least one of the endpoints of $e$ is in $C$.\\
Definition: A minimum vertex cover is a vertex cover of smallest possible size.\\
Input: A graph $G = (V, E)$\\
Output: A minimum vertex cover\\
\textbf{Example}
\begin{center}
    \begin{tikzpicture}
        \node[shape=circle,draw=black] (A) at (0, 0) {A};
        \node[shape=circle,draw=black] (B) at (2, 0) {B};
        \node[shape=circle,draw=black] (C) at (4, 2) {C};
        \node[shape=circle,draw=black] (D) at (4, -2) {D};
        \node[shape=circle,draw=black] (E) at (6, 0) {E};
        \node[shape=circle,draw=black] (F) at (6, 4) {F};
        \draw (A) -- (B);
        \draw (A) -- (C);
        \draw (A) -- (D);
        \draw (B) -- (C);
        \draw (B) -- (D);
        \draw (C) -- (D);
        \draw (C) -- (E);
        \draw (C) -- (F);
        \draw (D) -- (E);
        \draw (E) -- (F);
    \end{tikzpicture}\\
    the vertex cover $\{A, C, E\}$ is not a minimum vertex cover\\
    the minimum vertex cover is $\{A, E\}$.
\end{center}
\textbf{Vertex cover is NP-hard}: construct a polynomial time reduction from 3 SAT to vertex cover.\\
\indent Let $\varphi$ be a 3 CNF formula with $m$ clauses and $d$ variables.\\
\indent Construct in polynomial time an instance $<G, k>$ of vertex cover, with $k = d + 2m$.\\
\indent \indent if $\varphi$ is satisfiable, then $G$ has a vertex cover of size at most $k$\\
\indent \indent \indent Let $(y_1, y_2, \dots, y_d)$ in $\{0, 1\}^d$ be a satisfying assignment for $\varphi$\\
\indent \indent \indent For the nodes on the top: If $y_i = 1$. Include node $x_i$ in the vertex cover $C$, otherwise include node $\lnot x_i$ in $C$.\\
\indent \indent \indent For the nodes on the bottom: in each triangle, choose a node $x_i$ that has been picked on the top and do not include it in the vertex cover. Include the other two nodes.\\
\indent \indent if $\varphi$ is not satisfiable, then $G$ has no vertex cover of size at most $k$\\
\indent \indent \indent Let $C$ be a vertex cover of size $k = d + 2m$ in $G$.\\
\indent \indent \indent Since it is a vertex cover, it must include at least two out of three nodes in each "clause gadget" at the bottom.\\
\indent \indent \indent this means that at most $d$ nodes can be picked on the top.\\
\indent \indent \indent To satisfy the edges at the top, in each "variable gadget", at least one node must be picked.\\

\section{Further reductions in NP}
\textbf{Form optimization to decision}\\
Given an optimization problem $P$, introcude a threshold $k$.\\
The decision version $P_d$ becomes: Given an instance of $P$ and the threshold $k$ as input, is there a solution to $P$ with value at most $k$?\\
\indent If $P$ solved in polynomial time, then $P_d$ is also solved in polynomial time.\\
\indent If $P_d$ solved in polynomial time, then $P$ is also solved in polynomial time.\\












































\end{document}